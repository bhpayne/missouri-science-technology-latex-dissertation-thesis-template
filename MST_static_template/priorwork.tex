\section{Prior Work} \label{sec:previous}

There have been many successful attempts at using parameter control to reduce or eliminate the burden of selecting parameters a priori while maintaining or improving the performance of an EA~\cite{paramsetting}. However, very little work has been done in regard to dynamically controlling $\lambda$. Hansen et al.~\cite{hansen:onsizing} devised a method for adjusting $\lambda$ for (1,$\lambda$) Evolutionary Strategies (ESs) based on the second best individual created and the $\lambda$ used last generation.  The goal of this strategy is to maximize the local serial progress-rate, i.e., the expected fitness gain per fitness evaluation.  However, maximizing the local serial progress-rate is equivalent to maximizing the convergence rate, which often leads to premature convergence on complex fitness landscapes.

Jansen et al.~\cite{jansen:choice} created a method for adjusting  $\lambda$ for (1+$\lambda$) ESs based on the number of offspring that are fitter than their parent.  When none of the offspring created during a generation are fitter than their parent, $\lambda$ is doubled; otherwise, $\lambda$ is divided by the number of offspring that are fitter than the parent.  The idea being that $\lambda$ is increased quickly when it appears to be too small, and decreased based on the current success rate when it appears to be too large.  While this approach was empirically shown to work well for less complex fitness landscapes, it had problems with more complex fitness landscapes that require a larger value for $\lambda$.  On a complex fitness landscape, a larger $\lambda$ is required to ensure that successful offspring lie on the path to the global optimum.  However, it is impossible to know how many offspring lie on this path without a priori knowledge of the fitness landscape.  So while the method proposed by Jansen et. al. does allow for  $\lambda$ to increase quickly, they concluded that there is no way of knowing how large $\lambda$ needs to grow a priori without using knowledge of the problem.  Thus, $\lambda$ grew to smaller, suboptimal values instead.

The aforementioned methods were created for ESs with a population size, $\mu$, of one.  Although it is possible that these methods could be generalized for any population size, these methods inherently have drawbacks that prevent them from being well-suited for complex fitness landscapes.  The Hansen method prematurely converges due to its goal of maximizing the convergence rate, and the Jansen method tends to grow $\lambda$ to smaller, suboptimal values.  These deficiencies are independent from $\mu$.  
%FuBOS, however,  is not restricted to ESs or population sizes of one, and is suited for complex fitness landscapes as shown in this paper.

In~\cite{hansen:onsizing}, the concept of a ``cut-off point'' is introduced.  This is the point during the mating process where having additional offspring will result in additional cost with negligible benefit to the search, i.e., the additional search space explored by having an additional offspring is not worth the computational effort required to create the offspring and evaluate its fitness. While determining the cut-off point can be done a priori employing asymptotical analysis for certain situations \cite{hansen:onsizing}, it is usually far more difficult and time-consuming to do this than to manually tune $\lambda$. However, this idea can be used to determine how many offspring should be created during any given generation.